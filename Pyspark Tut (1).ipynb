{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Getting Started with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import difflib\n",
    "import datetime\n",
    "\n",
    "#sc is the sparkContext that gets created whe pyspark is started\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If SparkContext doesnot get Intialized by itself, following set of commands may be used:\n",
    "\n",
    "import findspark <br>\n",
    "findspark.init() <br>\n",
    "import pyspark # only run after findspark.init() <br>\n",
    "from pyspark.sql import SparkSession <br>\n",
    "spark = SparkSession.builder.getOrCreate() <br>\n",
    "import pandas as pd <br>\n",
    "sc = spark.sparkContext <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Make a sample dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using pandas Dataframe:\n",
    "\n",
    "Read Data into Pandas from csv, txt, excel files, etc. <br> \n",
    "Define a Schema: columns and their data types <br>\n",
    "Use CreateDataFrame() to convert to spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Gender': {0:'M', 1: 'F', 2: 'F', 3: 'F', 4: 'M'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
    "\n",
    "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without defining Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = hc.createDataFrame(df1_pd)\n",
    "df2 = hc.createDataFrame(df2_pd)\n",
    "#df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Defined Data Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_schema_1 = StructType([StructField('Gender',StringType(),True),\n",
    "                          StructField('PassengerId',IntegerType(),True),\n",
    "                          StructField('Name',StringType(),True),\n",
    "                          StructField('Survived',IntegerType(),True) ])\n",
    "# for date type: StructField('MeetingDate',DateType(),True) ])\n",
    "#True denotes that data field is nullable\n",
    "df_spark_1 = hc.createDataFrame(df1_pd,df_schema_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the same for df2_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Using Hadoop Tables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By importing the whole table: hc.table('table name')\n",
    "#cust_1: database name in hadoop; #abc_xxx : table name\n",
    "data = hc.table('cust_1.abc_xxx') \n",
    "# data is now a pyspark dataframe\n",
    "\n",
    "# By running a sql query:\n",
    "\n",
    "data = hc.sql(''' select name, r_no, age from cust_1.abc_xxx where trim(age) = '30' ''')\n",
    "\n",
    "#data is now a pyspark dataframe\n",
    "\n",
    "# To Note: trim is used in string types to ensure removal of white spaces, since data in hive can be unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Loading From CSV\n",
    "\n",
    "We can load our data from a CSV file in an S3 bucket. There are three ways to handle data types (dtypes) for each column: The easiest, but the most computationally-expensive, is to pass inferSchema=True to the load method. The second way entails specifiying the dtypes manually for every column by passing schema=StructType(...), which is computationally-efficient but may be difficult and prone to coder error for especially wide datasets. The final option is to not specify a schema option at all, in which case Spark will assign all the columns string dtypes. Note that dtypes can be changed later, as we will demonstrate, though it is more costly than doing it correctly in the loading process.\n",
    "\n",
    "Loading the data with the schema inferred:\n",
    "\n",
    "Note: Check this link for the file: https://github.com/UrbanInstitute/pyspark-tutorials/blob/master/01_pyspark-basics-1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').options(header='False', inferschema='true', sep='|').\\\n",
    "load('s3://ui-spark-social-science-public/data/Performance_2015Q1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Example loading of the same data by passing a custom schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    " from pyspark.sql.types import DateType, TimestampType, IntegerType, FloatType, LongType, DoubleTypefrom py \n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "custom_schema = StructType([StructField('_c0', DateType(), True),\n",
    "                           StructField('_c1', StringType(), True),\n",
    "                           StructField('_c2', DoubleType(), True),\n",
    "                           StructField('_c3', DoubleType(), True),\n",
    "                           StructField('_c4', DoubleType(), True),\n",
    "                           StructField('_c5', IntegerType(), True),\n",
    "                           ...\n",
    "                           StructField('_c27', StringType(), True)])\n",
    "                           \n",
    "df = spark.read.csv('s3://ui-spark-social-science-public/data/Performance_2015Q1.txt', header=False, schema=custom_schema, sep='|')\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of using infering and specifying a schema together might be with a large, unfamiliar dataset that you know you will need to load up and work with repeatedly. The first time you load it use inferSchema, then make note of the dtypes it assigns. Use that information to build the custom schema, so that when you load the data in the future you avoid the extra processing time necessary for infering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interesting read to use persist() to improve computation speed by branching : https://github.com/UrbanInstitute/pyspark-tutorials/blob/master/02_pyspark-basics-2.ipynb\n",
    "\n",
    "https://github.com/UrbanInstitute/pyspark-tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the titanic data going further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Change DataFrames Properties\n",
    "#### 2.1 Change Column Type\n",
    " Available types include <br>\n",
    "\n",
    "BinaryType <br>\n",
    "BooleanType <br>\n",
    "ByteType <br>\n",
    "DoubleType <br>\n",
    "DateType <br>\n",
    "FloatType <br>\n",
    "IntegerType <br>\n",
    "etc.\n",
    "\n",
    "#### Q. Change data type of PassengerID to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# .withColumn return a DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "df_spark_2 = df_spark_1.withColumn(\"PassengerId\", df_spark_1[\"PassengerId\"].cast(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gender', 'string'),\n",
       " ('PassengerId', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Survived', 'int')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Change Column Name\n",
    "\n",
    "#### Q. Rename Gender Column to Sex and Name to Passenger_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_spark_3 = df_spark_2.withColumnRenamed(\"Gender\", \"Sex\").withColumnRenamed(\"Name\", \"Passenger_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Passenger_Name: string (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_spark_3 = df_spark_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Modify Column Values and create new columns\n",
    "\n",
    "#### Q1. Modify values in Sex Column: M to Male and F to female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_spark_4 = df_spark_3.withColumn(\"Sex\", F.when(trim(df_spark_3['Sex']) == 'M', 'Male').\\\n",
    "                                   otherwise(F.when(trim(df_spark_3['Sex']) == 'F', 'Female').otherwise('NA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Sex: string, PassengerId: string, Name: string, Survived: int]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Q2. Create New column 'type_of_data' with constant value: 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_spark_5 = df_spark_4.withColumn('type_of_data', lit('train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Order DataFrame by Specified Column(s)\n",
    "\n",
    "#### Q. Order by  descending order: PassengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+--------+\n",
      "|   Sex|PassengerId|    Name|Survived|\n",
      "+------+-----------+--------+--------+\n",
      "|  Male|          5| William|       0|\n",
      "|Female|          4|    Lily|       1|\n",
      "|Female|          3|   Laina|       1|\n",
      "|Female|          2|Florence|       1|\n",
      "|  Male|          1|    Owen|       0|\n",
      "+------+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_4.sort(df_spark_4.PassengerId.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+--------+\n",
      "|   Sex|PassengerId|    Name|Survived|\n",
      "+------+-----------+--------+--------+\n",
      "|  Male|          5| William|       0|\n",
      "|Female|          4|    Lily|       1|\n",
      "|Female|          3|   Laina|       1|\n",
      "|Female|          2|Florence|       1|\n",
      "|  Male|          1|    Owen|       0|\n",
      "+------+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_4.sort('PassengerId', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Filter by Specified Column(s)\n",
    "\n",
    "#### Q. Filter out the female population from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter takes column expression or SQL expression ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Using Column expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_data = df_spark_4.filter(trim(col('Sex')) == 'Female')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  col('columnname') is eqivalent to dataframe['columnname'] equivalent to dataframe.columnname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Using SQL expression.Note the double and single quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_data_2 = df_spark_4.filter(\"Sex='Female'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Summarize/Aggregate and group by\n",
    "\n",
    "#### Q. Group By Passenger Class and find avergae Fare and Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2 = df2.groupby('Pclass')\n",
    "\n",
    "avg_cols = ['Age', 'Fare']\n",
    "gdf2.avg(*avg_cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Group By Passenger Class and find Total Fare and Average Age and count of passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     1|       2|              36.5|    124.4|\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. rename the columns count(1), avg(Age) etc, \n",
    "using toDF()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     1|     2|              36.5|     124.4|\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\\\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Arrange passenger class by total fare in ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Pclass|sum(Fare)|\n",
      "+------+---------+\n",
      "|     3|     23.2|\n",
      "|     1|    124.4|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pclass').agg({'Fare':'sum'}).sort('sum(Fare)', ascending = True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Get Count of distinct Passenger classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select('Pclass').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Pclass|\n",
      "+------+\n",
      "|     1|\n",
      "|     3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('Pclass').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Get Count of 'Distinct' Passenger Ids by Sex\n",
    "\n",
    "#####Note 1: countDistinct here is a SQL Function hence 'F.countDistinct'; <br>\n",
    "#####Note 2: Like F.countDistinct, F.sum, F.avg, F.max, etc sql functions can be used. <br>\n",
    "#####Note 3: here there is no repitition of passenger ids hence count gives same result as count distinct function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|   Sex|count(PassengerId)|\n",
      "+------+------------------+\n",
      "|Female|                 3|\n",
      "|  Male|                 2|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_4.groupby('Sex').agg(F.countDistinct('PassengerId')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Pclass|sum(Fare)|\n",
      "+------+---------+\n",
      "|     1|    124.4|\n",
      "|     3|     23.2|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pclass').agg(F.sum('Fare')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Joins and unions¶\n",
    "####There are two ways to combine dataframes --- joins and unions. The idea here is the same as joining and unioning tables in SQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Joins¶ Q.Join the two titanic dataframes by the column PassengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------+--------+----+---+------+\n",
      "|PassengerId|Gender|    Name|Survived|Fare|Age|Pclass|\n",
      "+-----------+------+--------+--------+----+---+------+\n",
      "|          1|     M|    Owen|       0| 7.3| 22|     3|\n",
      "|          2|     F|Florence|       1|71.3| 38|     1|\n",
      "|          3|     F|   Laina|       1| 7.9| 26|     3|\n",
      "|          4|     F|    Lily|       1|53.1| 35|     1|\n",
      "|          5|     M| William|       0| 8.0| 35|     3|\n",
      "+-----------+------+--------+--------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, ['PassengerId']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+--------+----+---+-----------+------+\n",
      "|Gender|PassengerId|    Name|Survived|Fare|Age|PassengerId|Pclass|\n",
      "+------+-----------+--------+--------+----+---+-----------+------+\n",
      "|     M|          1|    Owen|       0| 7.3| 22|          1|     3|\n",
      "|     F|          2|Florence|       1|71.3| 38|          2|     1|\n",
      "|     F|          3|   Laina|       1| 7.9| 26|          3|     3|\n",
      "|     F|          4|    Lily|       1|53.1| 35|          4|     1|\n",
      "|     M|          5| William|       0| 8.0| 35|          5|     3|\n",
      "+------+-----------+--------+--------+----+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, trim(df1['PassengerId']) == trim(df2['PassengerId']), 'left_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : Joins using condition create duplicate columns with same name; it creates problem later while storing back to hadoop database , etc hence better to drop one of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+--------+----+---+------+\n",
      "|Gender|PassengerId|    Name|Survived|Fare|Age|Pclass|\n",
      "+------+-----------+--------+--------+----+---+------+\n",
      "|     M|          1|    Owen|       0| 7.3| 22|     3|\n",
      "|     F|          2|Florence|       1|71.3| 38|     1|\n",
      "|     F|          3|   Laina|       1| 7.9| 26|     3|\n",
      "|     F|          4|    Lily|       1|53.1| 35|     1|\n",
      "|     M|          5| William|       0| 8.0| 35|     3|\n",
      "+------+-----------+--------+--------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, trim(df1['PassengerId']) == trim(df2['PassengerId']), 'left_outer').drop(df2['PassengerId']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonequi joins\n",
    "\n",
    "Here is an example of nonequi join. They can be very slow due to skewed data, but this is one operation that Spark can do while Hive can not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+--------+----+---+-----------+------+\n",
      "|Gender|PassengerId|    Name|Survived|Fare|Age|PassengerId|Pclass|\n",
      "+------+-----------+--------+--------+----+---+-----------+------+\n",
      "|     M|          1|    Owen|       0| 7.3| 22|          1|     3|\n",
      "|     M|          1|    Owen|       0|71.3| 38|          2|     1|\n",
      "|     F|          2|Florence|       1|71.3| 38|          2|     1|\n",
      "|     M|          1|    Owen|       0| 7.9| 26|          3|     3|\n",
      "|     M|          1|    Owen|       0|53.1| 35|          4|     1|\n",
      "|     M|          1|    Owen|       0| 8.0| 35|          5|     3|\n",
      "|     F|          2|Florence|       1| 7.9| 26|          3|     3|\n",
      "|     F|          2|Florence|       1|53.1| 35|          4|     1|\n",
      "|     F|          2|Florence|       1| 8.0| 35|          5|     3|\n",
      "|     F|          3|   Laina|       1| 7.9| 26|          3|     3|\n",
      "|     F|          3|   Laina|       1|53.1| 35|          4|     1|\n",
      "|     F|          3|   Laina|       1| 8.0| 35|          5|     3|\n",
      "|     F|          4|    Lily|       1|53.1| 35|          4|     1|\n",
      "|     F|          4|    Lily|       1| 8.0| 35|          5|     3|\n",
      "|     M|          5| William|       0| 8.0| 35|          5|     3|\n",
      "+------+-----------+--------+--------+----+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.PassengerId <= df2.PassengerId).show() # Note the duplicate col names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Unions\n",
    "Union() returns a dataframe from the union of two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+--------+\n",
      "|Gender|PassengerId|    Name|Survived|\n",
      "+------+-----------+--------+--------+\n",
      "|     M|          1|    Owen|       0|\n",
      "|     F|          2|Florence|       1|\n",
      "|     F|          3|   Laina|       1|\n",
      "|     F|          4|    Lily|       1|\n",
      "|     M|          5| William|       0|\n",
      "|     M|          1|    Owen|       0|\n",
      "|     F|          2|Florence|       1|\n",
      "|     F|          3|   Laina|       1|\n",
      "|     F|          4|    Lily|       1|\n",
      "|     M|          5| William|       0|\n",
      "+------+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionAll(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Similarly, df1.intersect(df2) can be used to get intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  4. Transform A DataFrame Column (using UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----------+------+-----------+\n",
      "|Fare|Age|PassengerId|Pclass|fare_3times|\n",
      "+----+---+-----------+------+-----------+\n",
      "| 7.3| 22|          1|     3|     $730.0|\n",
      "|71.3| 38|          2|     1|    $7130.0|\n",
      "| 7.9| 26|          3|     3|     $790.0|\n",
      "|53.1| 35|          4|     1|    $5310.0|\n",
      "| 8.0| 35|          5|     3|     $800.0|\n",
      "+----+---+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "derive_perc = udf(lambda x: \"$\" + str(x * 100) )\n",
    "# or \n",
    "# @udf\n",
    "# def derive_perc(x):\n",
    "#     return(str(x * 100, 3) + \"%\")\n",
    "\n",
    "thrice_fare = df2.withColumn(\"fare_3times\", derive_perc(df2['Fare']))\n",
    "\n",
    "thrice_fare.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using Spark SQL\n",
    "Reference: https://github.com/XD-DENG/Spark-practice/blob/master/Spark%20DataFrames%20%26%20SQL%20-%20Basics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a view with the Spark DataFrame, then we can SQL syntax to further process our data.\n",
    "\n",
    "You may notice there are two ways to build a view,\n",
    "\n",
    "DF.createGlobalTempView (or DF.createOrReplaceGlobalTempView): create a global temporary view\n",
    "DF.createTempView (or DF.createOrReplaceTempView): create a local temporary view\n",
    "The main difference between them is the lifetime. The lifetime of a global temporary view is tied to the Spark application, while lifetime of a local temporary view is tied to the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'package_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-ccd232ef4782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Creates or replaces a local temporary view with a DataFrame.# Create\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpackage_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"package_count_sql_table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'package_count' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creates or replaces a local temporary view with a DataFrame.# Create \n",
    "# The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame.\n",
    "package_count.createOrReplaceTempView(\"package_count_sql_table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic Spark SQL Query - 1\n",
    "query_result = sqlContext.sql(\"select perc \\\n",
    "                               from package_count_sql_table \\\n",
    "                               where package = 'DT'\")\n",
    "\n",
    "print(query_result.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic Spark SQL Query - 2\n",
    "query_result = sqlContext.sql(\"select * \\\n",
    "                                from package_count_sql_table \\\n",
    "                                where count > 1000 \\\n",
    "                                order by count asc\")\n",
    "print(query_result.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use the Spark RDD way to process the results from Spark SQL query result# Use th \n",
    "query_result.rdd.map(lambda x:x['package'] + \":\" + x['perc']).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### registering the dataframe as table in hadoop to use in select queries; act as temp tables"
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " hc.registerDataFrameAsTable(query_result, 'query_result_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = hc.sql('query_result_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to register /store as permamnent table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_result.write.option(\"sep\",\"|\").format('orc').mode(\"overwrite\").saveAsTable(\"Cust_xxx.nxxx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
