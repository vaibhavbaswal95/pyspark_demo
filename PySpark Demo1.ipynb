{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import difflib\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Dataframe from Dict of Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Gender': {0:'M', 1: 'F', 2: 'F', 3: 'F', 4: 'M'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
    "\n",
    "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use HiveContext to createDataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = hc.createDataFrame(df1_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = hc.createDataFrame(df2_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printing Schema of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe with defining Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_schema = StructType([StructField('PassengerId',IntegerType(),True),\n",
    "                         StructField('Name',StringType(),True),\n",
    "                         StructField('Gender',StringType(),True),\n",
    "                         StructField('Survived',IntegerType(),True)])\n",
    "# for date type: StructField('MeetingDate',DateType(),True) ])\n",
    "#True denotes that data field is nullable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_spark = hc.createDataFrame(df1_pd,df1_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|Gender|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|     M|       0|\n",
      "|          2|Florence|     F|       1|\n",
      "|          3|   Laina|     F|       1|\n",
      "|          4|    Lily|     F|       1|\n",
      "|          5| William|     M|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_schema = StructType([StructField('PassengerId',IntegerType(),True),\n",
    "                         StructField('Age',IntegerType(),True),\n",
    "                         StructField('Fare',DoubleType(),True),\n",
    "                         StructField('PClass',IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_spark = hc.createDataFrame(df2_pd,df2_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|PClass|\n",
      "+-----------+---+----+------+\n",
      "|          1| 22| 7.3|     3|\n",
      "|          2| 38|71.3|     1|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Datatypes available\n",
    "\n",
    "BinaryType, BooleanType, ByteType, DoubleType, DateType, FloatType, IntegerType "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#withColumn(ColName, Col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_spark_1 = df1_spark.withColumn(\"PassengerId\", df1_spark[\"PassengerId\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Column Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_spark = df1_spark.withColumnRenamed('Gender','Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying Column Values and Creating new Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M to Male and F to Female in Sex Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_spark_2 = df1_spark.withColumn('Sex',F.when(trim(df1_spark['Sex']) == 'M','Male')\\\n",
    "                                 .otherwise(F.when(trim(df1_spark['Sex']) == 'F','Female').otherwise('NA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  Male|       0|\n",
      "|          2|Florence|Female|       1|\n",
      "|          3|   Laina|Female|       1|\n",
      "|          4|    Lily|Female|       1|\n",
      "|          5| William|  Male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create New column 'type_of_data' with constant value: 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_spark_2 = df1_spark_2.withColumn('type_of_data',lit('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lit() creates a column of literal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+------------+\n",
      "|PassengerId|    Name|   Sex|Survived|type_of_data|\n",
      "+-----------+--------+------+--------+------------+\n",
      "|          1|    Owen|  Male|       0|       train|\n",
      "|          2|Florence|Female|       1|       train|\n",
      "|          3|   Laina|Female|       1|       train|\n",
      "|          4|    Lily|Female|       1|       train|\n",
      "|          5| William|  Male|       0|       train|\n",
      "+-----------+--------+------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering DataFrame by specific Column - PassengerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+------------+\n",
      "|PassengerId|    Name|   Sex|Survived|type_of_data|\n",
      "+-----------+--------+------+--------+------------+\n",
      "|          5| William|  Male|       0|       train|\n",
      "|          4|    Lily|Female|       1|       train|\n",
      "|          3|   Laina|Female|       1|       train|\n",
      "|          2|Florence|Female|       1|       train|\n",
      "|          1|    Owen|  Male|       0|       train|\n",
      "+-----------+--------+------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark_2.sort(df1_spark_2.PassengerId.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+------------+\n",
      "|PassengerId|    Name|   Sex|Survived|type_of_data|\n",
      "+-----------+--------+------+--------+------------+\n",
      "|          5| William|  Male|       0|       train|\n",
      "|          4|    Lily|Female|       1|       train|\n",
      "|          3|   Laina|Female|       1|       train|\n",
      "|          2|Florence|Female|       1|       train|\n",
      "|          1|    Owen|  Male|       0|       train|\n",
      "+-----------+--------+------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark_2.sort('PassengerId',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Specific Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out female population\n",
    "Filter takes column expression or SQL expression\n",
    "\n",
    "Using Columns Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_data = df1_spark_2.filter(trim(col('Sex'))=='Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+------------+\n",
      "|PassengerId|    Name|   Sex|Survived|type_of_data|\n",
      "+-----------+--------+------+--------+------------+\n",
      "|          2|Florence|Female|       1|       train|\n",
      "|          3|   Laina|Female|       1|       train|\n",
      "|          4|    Lily|Female|       1|       train|\n",
      "+-----------+--------+------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "female_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### col('columnname') = dataframe['columnname'] = dataframe.columnname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_data = df1_spark_2a.filter(\"Sex='Female'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+------------+\n",
      "|PassengerId|    Name|   Sex|Survived|type_of_data|\n",
      "+-----------+--------+------+--------+------------+\n",
      "|          2|Florence|Female|       1|       train|\n",
      "|          3|   Laina|Female|       1|       train|\n",
      "|          4|    Lily|Female|       1|       train|\n",
      "+-----------+--------+------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "female_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize / Aggregate and GroupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by Passenger class and find avg fare and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2_spark = df2_spark.groupby('PClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|PClass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_col = ['Age','Fare']\n",
    "gdf2_spark.avg(*avg_col).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group By Passenger Class and find Total Fare and Average Age and count of passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|PClass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     1|       2|              36.5|    124.4|\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2_spark.agg({'*':'count','Age':'Mean','Fare':'Sum'}).show() #avg/Avg/Mean sum/Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename the columns count(1), avg(Age) etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------+----------+\n",
      "|PClass|Count|           Avg Age|Total Fare|\n",
      "+------+-----+------------------+----------+\n",
      "|     1|    2|              36.5|     124.4|\n",
      "|     3|    3|27.666666666666668|      23.2|\n",
      "+------+-----+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2_spark.agg({'*':'count','Age':'Mean','Fare':'Sum'}).toDF('PClass','Count','Avg Age','Total Fare').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrange passenger class by total fare in ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|PClass|sum(Fare)|\n",
      "+------+---------+\n",
      "|     3|     23.2|\n",
      "|     1|    124.4|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2_spark.agg({'Fare':'Sum'}).sort('sum(Fare)',ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Count of distinct Passenger classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_spark.select('PClass').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Pclass|\n",
      "+------+\n",
      "|     1|\n",
      "|     3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_spark.select('Pclass').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Count of 'Distinct' Passenger Ids by Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1_spark = df1_spark_2.groupby('Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Like F.countDistinct, F.sum, F.avg, F.max, etc sql functions can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|   Sex|count(1)|\n",
      "+------+--------+\n",
      "|Female|       3|\n",
      "|  Male|       2|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf1_spark.agg({'*':'Count'}).show()a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+\n",
      "|   Sex|count(DISTINCT PassengerId)|\n",
      "+------+---------------------------+\n",
      "|Female|                          3|\n",
      "|  Male|                          2|\n",
      "+------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf1_spark.agg(F.countDistinct('PassengerId')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|PClass|sum(Fare)|\n",
      "+------+---------+\n",
      "|     1|    124.4|\n",
      "|     3|     23.2|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2_spark.agg(F.sum('Fare')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are two ways to combine dataframes --- joins and unions. The idea here is the same as joining and unioning tables in SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the two titanic dataframes by the column PassengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+--------+---+----+------+\n",
      "|PassengerId|    Name|Sex|Survived|Age|Fare|PClass|\n",
      "+-----------+--------+---+--------+---+----+------+\n",
      "|          1|    Owen|  M|       0| 22| 7.3|     3|\n",
      "|          3|   Laina|  F|       1| 26| 7.9|     3|\n",
      "|          5| William|  M|       0| 35| 8.0|     3|\n",
      "|          4|    Lily|  F|       1| 35|53.1|     1|\n",
      "|          2|Florence|  F|       1| 38|71.3|     1|\n",
      "+-----------+--------+---+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark.join(df2_spark,['PassengerId']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+--------+-----------+---+----+------+\n",
      "|PassengerId|    Name|Sex|Survived|PassengerId|Age|Fare|PClass|\n",
      "+-----------+--------+---+--------+-----------+---+----+------+\n",
      "|          3|   Laina|  F|       1|          3| 26| 7.9|     3|\n",
      "|          5| William|  M|       0|          5| 35| 8.0|     3|\n",
      "|          1|    Owen|  M|       0|          1| 22| 7.3|     3|\n",
      "|          4|    Lily|  F|       1|          4| 35|53.1|     1|\n",
      "|          2|Florence|  F|       1|          2| 38|71.3|     1|\n",
      "+-----------+--------+---+--------+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark.join(df2_spark,trim(df1_spark['PassengerID'])==trim(df2_spark['PassengerID']),'left_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : Joins using condition create duplicate columns with same name; it creates problem later while storing back to hadoop database , etc hence better to drop one of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+--------+---+----+------+\n",
      "|PassengerId|    Name|Sex|Survived|Age|Fare|PClass|\n",
      "+-----------+--------+---+--------+---+----+------+\n",
      "|          3|   Laina|  F|       1| 26| 7.9|     3|\n",
      "|          5| William|  M|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  M|       0| 22| 7.3|     3|\n",
      "|          4|    Lily|  F|       1| 35|53.1|     1|\n",
      "|          2|Florence|  F|       1| 38|71.3|     1|\n",
      "+-----------+--------+---+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark.join(df2_spark, trim(df1_spark['PassengerId']) == trim(df2_spark['PassengerId']), 'left_outer').drop(df2_spark['PassengerId']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonequi joins - with conditions like >,>=,<,<="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+--------+-----------+---+----+------+\n",
      "|PassengerId|    Name|Sex|Survived|PassengerId|Age|Fare|PClass|\n",
      "+-----------+--------+---+--------+-----------+---+----+------+\n",
      "|          2|Florence|  F|       1|          1| 22| 7.3|     3|\n",
      "|          3|   Laina|  F|       1|          1| 22| 7.3|     3|\n",
      "|          3|   Laina|  F|       1|          2| 38|71.3|     1|\n",
      "|          4|    Lily|  F|       1|          1| 22| 7.3|     3|\n",
      "|          5| William|  M|       0|          1| 22| 7.3|     3|\n",
      "|          4|    Lily|  F|       1|          2| 38|71.3|     1|\n",
      "|          5| William|  M|       0|          2| 38|71.3|     1|\n",
      "|          4|    Lily|  F|       1|          3| 26| 7.9|     3|\n",
      "|          5| William|  M|       0|          3| 26| 7.9|     3|\n",
      "|          5| William|  M|       0|          4| 35|53.1|     1|\n",
      "+-----------+--------+---+--------+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark.join(df2_spark,trim(df1_spark['PassengerId'])>trim(df2_spark['PassengerId'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+--------+\n",
      "|PassengerId|    Name| Sex|Survived|\n",
      "+-----------+--------+----+--------+\n",
      "|          1|    Owen|   M|       0|\n",
      "|          2|Florence|   F|       1|\n",
      "|          3|   Laina|   F|       1|\n",
      "|          4|    Lily|   F|       1|\n",
      "|          5| William|   M|       0|\n",
      "|          1|      22| 7.3|       3|\n",
      "|          2|      38|71.3|       1|\n",
      "|          3|      26| 7.9|       3|\n",
      "|          4|      35|53.1|       1|\n",
      "|          5|      35| 8.0|       3|\n",
      "+-----------+--------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_spark.union(df2_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform A DataFrame Column (using UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+------------+\n",
      "|PassengerId|Age|Fare|PClass|fare_3_times|\n",
      "+-----------+---+----+------+------------+\n",
      "|          1| 22| 7.3|     3|      $730.0|\n",
      "|          2| 38|71.3|     1|     $7130.0|\n",
      "|          3| 26| 7.9|     3|      $790.0|\n",
      "|          4| 35|53.1|     1|     $5310.0|\n",
      "|          5| 35| 8.0|     3|      $800.0|\n",
      "+-----------+---+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "derive_perc = udf(lambda x: '$'+str(x*100))\n",
    "\n",
    "df2_spark_1 = df2_spark.withColumn('fare_3_times',derive_perc(df2_spark['Fare']))\n",
    "\n",
    "df2_spark_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- yet to implement in my notebook ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
